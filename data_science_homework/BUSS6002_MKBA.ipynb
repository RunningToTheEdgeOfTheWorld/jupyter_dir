{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing and Business Analytics Individual Assignment\n",
    "\n",
    "1. This assignment accounts for 22% of your final mark.\n",
    "    - 15% from the Marketing component\n",
    "    - 7% from the Business Analytics component\n",
    "1. This is an individual assignment.\n",
    "2. The assignment is due before **17:00 26th October 2018**. The late penalty for the assignment is 10% of the assigned mark per day, starting after 5pm on the due date. The closing date **5pm 2nd November 2018** is the last date on which an assessment will be accepted for marking. \n",
    "3. Please only include your student ID in the submitted report, and do **NOT** include your name.\n",
    "\n",
    "## Background\n",
    "\n",
    "You are employed as a Data Scientist at a telecommunications company. One of the biggest issues facing the company is minimising customer churn. Customer churn is when a customer changes provider. The business is interested in analysing and predicting churn since the cost of acquiring new customers is higher than retaining existing customers.\n",
    "\n",
    "This is particularly problematic for the telecommunications industry as changing telecommunications provider is relatively easy. There are also a large number of price competitive providers to choose from, which encourages churning. \n",
    "\n",
    "Your job is to help the marketing department to undertake the following:\n",
    "- Investigate why and which customers churn\n",
    "- Discover retention oppurtunities and strategies\n",
    "- Identify current customers that are likely to churn so that the retention strategy can be applied\n",
    "- Identify potential customers so that the incentive strategy can be applied\n",
    "\n",
    "## Files\n",
    "\n",
    "Part 1:\n",
    "- churn_survey.json\n",
    "\n",
    "Part 2:\n",
    "- tweets.db\n",
    "\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "This assessment will be automatically marked. Any deviation from the stated output form will be marked as an incorrect answer. It is your responsibility to check that the output matches the given template or example for each question.\n",
    "\n",
    "#### What to Submit\n",
    "\n",
    "Submit only your .ipynb file. You can choose to use this file as a template OR start from scratch. Just make sure that running your notebook generates all the answers!\n",
    "\n",
    "#### Filename\n",
    "\n",
    "The filename must be \"BUSS6002_MKBA_STUDENTID.ipynb\"\n",
    "\n",
    "#### Loading the data files when marking\n",
    "\n",
    "We will run your notebook in the same directory as the data files. We will assume the original file names.\n",
    "\n",
    "#### Output\n",
    "\n",
    "The output for each question should be saved to the same directory as the notebook.\n",
    "\n",
    "#### Checking the format of your output files\n",
    "\n",
    "We have created ED Challenges for each question. The challenges will tell you if the FORMAT of your output file is correct. It does not tell you if your answer is correct. Please test your output files on Ed before submitting your assignment.\n",
    "\n",
    "#### Timeout\n",
    "\n",
    "We will automatically run your notebook. Each notebook will be given a maximum of 1 minute to be completed. Please ensure any model training or optimisation will be easily completed in this time frame.\n",
    "\n",
    "## Marking Criteria\n",
    "1. Correctness of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your Student ID here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOURSTUDENTID = 460490151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to save answers with a non-tabular output\n",
    "import pandas as pd\n",
    "\n",
    "def write_txt(part_number, data):\n",
    "    with open(\"ID_{0}_Q_{1}.txt\".format(YOURSTUDENTID, part_number), 'w') as file:\n",
    "        file.write(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Understanding Churn and Identifying Retention Strategies (20 Marks)\n",
    "\n",
    "## Data\n",
    "\n",
    "The marketing team has collected information from a subset of current and past customers. To understand why customers churn and identify why customers have churned use the ``churn_survey.json`` file.\n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "| Variable  | Description  |\n",
    "|---|---|\n",
    "| Churn  | Whether the customer churned or not |\n",
    "| Contract  | The contract term of the customer |\n",
    "| Dependents  | Whether the customer has dependents or not |\n",
    "| DeviceProtection  | Whether the customer has purchased the device protection service or not |\n",
    "| Gender  | Customer gender |\n",
    "| InternetService  | Customer’s internet service type |\n",
    "| MonthlyCharges  | The amount charged to the customer monthly |\n",
    "| MultipleLines  | Whether the customer has multiple lines or not |\n",
    "| OnlineBackup  | Whether the customer has purchased the additional online backup service or not |\n",
    "| OnlineSecurity  | Whether the customer has purchased the additional online security service or not |\n",
    "| PaperlessBilling  | Whether the customer has paperless billing or not |\n",
    "| Partner  | Whether the customer has a partner or not |\n",
    "| PaymentMethod  | The customer’s payment method |\n",
    "| PhoneService  | Whether the customer has a phone service or not |\n",
    "| SeniorCitizen  | Whether the customer is a senior citizen or not |\n",
    "| StreamingMovies  | Whether the customer has purchased the additional streaming movie service or not |\n",
    "| StreamingTV  | Whether the customer has purchased the additional streaming TV service or not |\n",
    "| TechSupport  | Whether the customer has purchased the additional tech support service or not |\n",
    "| Tenure  | Number of months the customer has stayed with the company |\n",
    "| TotalCharges  | The total amount charged to the customer |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this area to load the data\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### 1.1 What is the most popular payment method? (1 Mark)\n",
    "\n",
    "Output your answer as a .txt file containing the name of the most popular payment method.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOURSTUDENTID = 460490151\n",
    "\n",
    "# This function is used to save answers with a non-tabular output\n",
    "def write_txt(student_id, part_number, data):\n",
    "    file = open(\"ID_{0}_Q_{1}.txt\".format(student_id, part_number), 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\t\n",
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "method_name = data.PaymentMethod.value_counts().index[0]\n",
    "\n",
    "# This will save your answer to a .txt file\n",
    "write_txt(YOURSTUDENTID, \"1_1\", method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What is the mean amount spent per month for the churn and non-churn customers? (1 Mark)\n",
    "\n",
    "Output your answer as a .csv file with the following format to four decimal places. DO NOT include the $ sign.\n",
    "\n",
    "| Churn  | MonthlyCharges  |\n",
    "|---|---|\n",
    "| No  | 00.0000  |\n",
    "| Yes  |  00.0000 |\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "churn_month = data[['Churn', 'MonthlyCharges']]\n",
    "res_1_2 = churn_month.groupby('Churn').mean().round(4)\n",
    "res_1_2.to_csv('ID_460490151_Q_1_2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What is the standard deviation of amount spent per month for the churn and non-churn customers? (1 Mark)\n",
    "\n",
    "Output your answer as a .csv file with the following format to four decimal places. DO NOT include the $ sign.\n",
    "\n",
    "| Churn  | MonthlyCharges  |\n",
    "|---|---|\n",
    "| No  | 00.0000  |\n",
    "| Yes  |  00.0000 |\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_3.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MonthlyCharges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churn</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>31.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>25.0650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MonthlyCharges\n",
       "Churn               \n",
       "No           31.1460\n",
       "Yes          25.0650"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "churn_month = data[['Churn', 'MonthlyCharges']]\n",
    "std_month_spend = churn_month.groupby('Churn').std()\n",
    "std_month_spend.MonthlyCharges = std_month_spend.MonthlyCharges.map(lambda i: '{:.4f}'.format(i))\n",
    "std_month_spend.to_csv('ID_460490151_Q_1_3.csv')\n",
    "std_month_spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 What is the percentage of contract type for the churn and non-churn customers (2 Marks)\n",
    "\n",
    "Output your answer as a .csv file with the following format to two decimal places. Do not include the % symbol.\n",
    "\n",
    "| Churn  | Month-to-month  | One year | Two year |\n",
    "|---|---|---|---|\n",
    "| No  | 00.00  |00.00  |00.00  |\n",
    "| Yes  |  00.00 |00.00  |00.00  |\n",
    "\n",
    "This percentage should be relative to the churn status NOT the entire sample i.e. the top left cell is the percentage of customers on month-to-month contracts who didn't churn.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month-to-month</th>\n",
       "      <th>One year</th>\n",
       "      <th>Two year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Churn</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month-to-month One year Two year\n",
       "Churn                                 \n",
       "No              0.44     0.25     0.31\n",
       "Yes             0.89     0.08     0.03"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "contrace_type = list(data.Contract.drop_duplicates().sort_values())\n",
    "if_churn = data.Churn.drop_duplicates().dropna().sort_values()\n",
    "\n",
    "c_type_result = pd.DataFrame(index=if_churn, columns=contrace_type)\n",
    "\n",
    "c_count = data[data.columns[0:3]].groupby(['Churn', 'Contract']).count()\n",
    "for c in if_churn:\n",
    "    c_target = c_count.loc[(c, slice(None)), :]\n",
    "    c_count.loc[(c, slice(None)), :] = c_target/c_target.sum()\n",
    "\n",
    "for c in contrace_type:\n",
    "    for ic in if_churn:\n",
    "        c_type_result.loc[ic, c] = c_count.loc[(ic, c), 'Dependents'].round(2)\n",
    "c_type_result.to_csv('ID_460490151_Q_1_4.csv')\n",
    "c_type_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Which feature and value is most correlated with MonthlyCharges? (2.5 Marks)\n",
    "\n",
    "Output your answer as a .csv file with the following format containing the most correlated feature name and value.\n",
    "\n",
    "| Feature  | Value  |\n",
    "|---|---|---|---|\n",
    "| FEATURE_NAME  | FEATURE_VALUE  |\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TotalCharges</th>\n",
       "      <td>0.645966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Value\n",
       "Feature               \n",
       "TotalCharges  0.645966"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "def get_trans_data(data):\n",
    "    data_copy = data.copy()\n",
    "\n",
    "    for c in data.columns:\n",
    "        v_assets = {}\n",
    "        init_num = list(range(100))\n",
    "        def v_type_trans(v):\n",
    "            if isinstance(v, (int, float)):\n",
    "                return v\n",
    "            if v not in v_assets:\n",
    "                v_assets[v] = init_num.pop(0)\n",
    "            return v_assets[v]\n",
    "        data_copy.loc[:, c] = data_copy.loc[:, c].map(v_type_trans)\n",
    "    return data_copy\n",
    "data_copy = get_trans_data(data)\n",
    "\n",
    "corr_charges = np.abs(data_copy.corr().MonthlyCharges).sort_values()\n",
    "\n",
    "res_1_5 = pd.DataFrame({'Feature': [corr_charges.index[-2]], \"Value\": [corr_charges[-2]]})\n",
    "res_1_5 = res_1_5.set_index('Feature')\n",
    "res_1_5.to_csv('ID_460490151_Q_1_5.csv')\n",
    "res_1_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 What is the count, mean, std, min, quartiles and max of time before a customer churns? (2.5 Marks)\n",
    "\n",
    "Output your result as a two column .csv with the following format to four decimal places\n",
    "\n",
    "|   | Tenure  |\n",
    "|---|---|\n",
    "| count | 0.0 |\n",
    "| mean  | 0.0 |\n",
    "| std   | 0.0 |\n",
    "| min  | 0.0 |\n",
    "| 25%  | 0.0 |\n",
    "| 50%  | 0.0 |\n",
    "| 75%  | 0.0 |\n",
    "| max  | 0.0 |\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_6.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tenure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>933.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.8132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>72.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tenure\n",
       "count  933.0000\n",
       "mean    18.1994\n",
       "std     19.8132\n",
       "min      1.0000\n",
       "25%      2.0000\n",
       "50%     10.0000\n",
       "75%     30.0000\n",
       "max     72.0000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "chrun_data = data[data.Churn == 'Yes'].Tenure\n",
    "res_1_6 = pd.DataFrame(chrun_data.describe().round(4))\n",
    "res_1_6.to_csv('ID_460490151_Q_1_6.csv')\n",
    "res_1_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 What is the proportion of purchase for each account addon for male and female customers? (4 Marks)\n",
    "\n",
    "Output your result as a .csv with the following format to four decimal places\n",
    "\n",
    "| Gender  | ADDON1  | ADDON2  | ... |\n",
    "|---|---|---|\n",
    "| Female  | 0.0000  | 0.0000  | .. |\n",
    "| Male  |  0.0000 | 0.0000  | .. |\n",
    "\n",
    "Please use the original name of the addon from the data. You must use your understanding of the data and the problem to determine where you can find this information in the dataset.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>TechSupport</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.5063</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.5228</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.5074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.4937</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.4824</td>\n",
       "      <td>0.4946</td>\n",
       "      <td>0.4926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DeviceProtection OnlineBackup OnlineSecurity StreamingMovies  \\\n",
       "Gender                                                                \n",
       "Female           0.5063       0.5130         0.5228          0.5176   \n",
       "Male             0.4937       0.4870         0.4772          0.4824   \n",
       "\n",
       "       StreamingTV TechSupport  \n",
       "Gender                          \n",
       "Female      0.5054      0.5074  \n",
       "Male        0.4946      0.4926  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "no_intern_user = data.loc[5, :]\n",
    "addons = list(no_intern_user[no_intern_user == 'No internet service'].index)\n",
    "addons\n",
    "data_copy = data.copy()\n",
    "gen_count = (data_copy.set_index('Gender')[addons].replace('No internet service', 'No') == 'Yes').groupby('Gender').sum().astype(np.int)\n",
    "res_1_7 = gen_count.apply(lambda i: i/i.sum()).round(4)\n",
    "res_1_7 = res_1_7.applymap(lambda i: '{:.4f}'.format(i))\n",
    "res_1_7.to_csv('ID_460490151_Q_1_7.csv')\n",
    "res_1_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Of the listed account addons, which addon/addons could be offered to churning customers for free or at a discounted rate in order to best retain them? (6 Marks)\n",
    "\n",
    "Output your file as a single column .csv with the following format\n",
    "\n",
    "| Addon  |\n",
    "|---|\n",
    "| ADDONX  | \n",
    "| ADDONY  |\n",
    "| ...  | \n",
    "\n",
    "where ADDONX is the name of one addon that you suggest. You must suggest at least 1 account addon up the total amount of addons listed in the dataset. You must exercise your best judgement and supporting evidence from the data to obtain a list of suggested addons. These addons should reflect the interests of the churning customers, i.e. which addons they actually care about.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_1_8.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Addon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StreamingTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StreamingMovies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Addon\n",
       "0      StreamingTV\n",
       "1  StreamingMovies"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('churn_survey.json') as f:\n",
    "    data = json.load(f)\n",
    "data = pd.DataFrame(data)\n",
    "no_intern_user = data.loc[5, :]\n",
    "addons = list(no_intern_user[no_intern_user == 'No internet service'].index)\n",
    "\n",
    "addons_data = data[addons]\n",
    "addons_data = addons_data[addons_data.DeviceProtection != 'No internet service']\n",
    "interest_addons = (addons_data == 'Yes').sum().sort_values()[-2:][::-1]\n",
    "res_1_8 = pd.DataFrame({\"Addon\": list(interest_addons.index)})\n",
    "res_1_8.to_csv( 'ID_460490151_Q_1_8.csv',index=False)\n",
    "\n",
    "res_1_8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Churn Intervention (24 Marks)\n",
    "\n",
    "The marketing teams wants to know if it is possible to identify customers on social media before they churn and if they can identify potential customers that want to move away from their existing provider. If a customer is identified before they churn then the retention strategy that you developed in Part 1.6 can be applied to stop the customer leaving.\n",
    "\n",
    "The marketing team has outsourced the collection of data and labels. The data was collected from twitter and includes tweets referencing your company and competitors. The data is available in the ``tweets.db`` SQLite file.\n",
    "\n",
    "To achieve the goals of the project you will need to do some EDA to understand the nature of the data and attempt to build a classifier to predict if an individual is likely to churn based on what they wrote in their tweet.\n",
    "\n",
    "## Data\n",
    "\n",
    "### Schema\n",
    "\n",
    "The schema for the ``tweets.db`` file is below:\n",
    "\n",
    "churn\n",
    "\n",
    "| Column  |  Description |\n",
    "|---|---|\n",
    "| tid  | Tweet ID  |\n",
    "| churn  | Churn status  |\n",
    "| set  | Training or Hidden  |\n",
    "\n",
    "tweets\n",
    "\n",
    "| Column  |  Description |\n",
    "|---|---|\n",
    "| tid  | Tweet ID  |\n",
    "| uid  | User ID  |\n",
    "| date  | Datetime of the tweet  |\n",
    "| text  | Content of the tweet  |\n",
    "\n",
    "### Training and Hidden Sets\n",
    "\n",
    "The data has been divided into two sets:\n",
    "\n",
    "| Set  |  Tweets | Target  |\n",
    "|---|---|---|\n",
    "| Training  | Yes  | Yes  |\n",
    "| Hidden  | Yes  | No  |\n",
    "\n",
    "The Churn labels for the training sets has been made available. However the marketing team wants to know how well your classifier will work on future and unseen data before deploying it. They will assess your classification performance on the hidden set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 How many tweets in the training set contain at least one of the strings \"AT&T\", \"Verizon\" or \"T-Mobile\" (1.5 Marks)\n",
    "\n",
    "Output the number of tweets as an integer to a .txt file. Your search should be invariant to capitilisation.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "YOURSTUDENTID = 460490151\n",
    "import pandas as pd\n",
    "\n",
    "def write_txt(part_number, data):\n",
    "    with open(\"ID_{0}_Q_{1}.txt\".format(YOURSTUDENTID, part_number), 'w') as file:\n",
    "        file.write(str(data))\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "\n",
    "sql = \"\"\"\n",
    "select count(*) from tweets join churn on churn.tid = tweets.tid \n",
    "where ((tweets.text like '%AT&T%') \n",
    "or (tweets.text like '%Verizon%')\n",
    "or (tweets.text like '%T-Mobile%'))\n",
    "and (churn.\"set\" like 'training')\n",
    "\"\"\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "tc_count = cursor.fetchall()[0][0]\n",
    "cursor.close()\n",
    "write_txt('2_1', tc_count)\n",
    "tc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Find all tweets in the training set containing the strings \"AT&T\", \"Verizon\" and \"switch\" (2.5 Marks)\n",
    "\n",
    "Output the tweets as a two column .csv file with the following format:\n",
    "\n",
    "| tid  |  text |\n",
    "|---|---|\n",
    "| tweet_id1  | text1  |\n",
    "| tweet_id2  | text2  |\n",
    "| tweet_id3  | text3  |\n",
    "| ...  | ...  |\n",
    "\n",
    "The first column should be the tweet id and the second column should be the original text of the tweet. Your search should be invariant to capitilisation.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>325958497122529280</td>\n",
       "      <td>Anybody wanna buy a Verizon iPhone 4s; for $125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324539657436143616</td>\n",
       "      <td>When will Nokia Lumia 920 at&amp;t exclusivity end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327288844758171648</td>\n",
       "      <td>at&amp;t! @Ms_Marshall: Im legit bouta switch back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347214658379317248</td>\n",
       "      <td>wind is a HUGE factor at at&amp;t park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>395868806867742720</td>\n",
       "      <td>My phone still fucked up. Taking this shit to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>388716746900647936</td>\n",
       "      <td>@SuperSteveHixon still want to assume those 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>398102041668509696</td>\n",
       "      <td>If the season ended today; the 49ers would be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>326852829237088256</td>\n",
       "      <td>Switching from at&amp;t to t mobile @ROLLthemTREESup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>301391249145991168</td>\n",
       "      <td>Fuck you; verizon. Im done with you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>431091671192076288</td>\n",
       "      <td>@Right_UpMaiale @Emma_Spic3 I keep going over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>343919820003221504</td>\n",
       "      <td>@Uppity1 If you cancel your Verizon service; t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>391309243703640064</td>\n",
       "      <td>@Crislex: Guess what @comcast; Im in the phone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>324296820572442624</td>\n",
       "      <td>Verizon;I would love to know why my bill is 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>312720777331998720</td>\n",
       "      <td>You have reached the Verizon voicemail of 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>311816375742128128</td>\n",
       "      <td>Just got off the phone with Verizon; that stup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>419274183030214656</td>\n",
       "      <td>at&amp;t is offering T-Mobile customers the opport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>420692453558321152</td>\n",
       "      <td>So Ive Been W/ at&amp;t for almost a week .. I lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>359362581955223552</td>\n",
       "      <td>at&amp;t be tryna get out on folks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>340587376080482304</td>\n",
       "      <td>My dream was to work at Verizon Wireless or Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>314918667018178560</td>\n",
       "      <td>@Net10_Wireless my cancellation fee from Veriz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>287013445403033600</td>\n",
       "      <td>@UcantSTANme: @gripthaGRAIN why you switching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>294941404872978432</td>\n",
       "      <td>@SuchA_EffnLadi no hes talking about dropping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>406799341559742464</td>\n",
       "      <td>@Pocketnow I will be switching to Verizon Prep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>424356419614158848</td>\n",
       "      <td>Final at&amp;t solution: Switch to data unlimited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>429851650820276224</td>\n",
       "      <td>@VZWSupport Been there; done that. Verizon ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>331058399049576448</td>\n",
       "      <td>I was thinking about paying that 215 but I ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>334491330476638208</td>\n",
       "      <td>end of 1st Q. Spurs have 37-28 lead vs GSW in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>383386026934165504</td>\n",
       "      <td>Verizon wireless would be so tight butthole if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>295616492219813888</td>\n",
       "      <td>i think im switching from verizon to metro onc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>416943791535575040</td>\n",
       "      <td>I cant wait til my contract up at at&amp;t Im sick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>310580189283373056</td>\n",
       "      <td>At the end of 2 periods of play; things are al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>320284000797200384</td>\n",
       "      <td>@ThierrymrFNF I wanna c if u can switch this v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>408037057508810752</td>\n",
       "      <td>at&amp;t RT@Amazing__dae: Who selling a iPhone 5 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>405889734343688192</td>\n",
       "      <td>Im on the other end of Verizon Center and can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>335434857201164288</td>\n",
       "      <td>@Love_HateB im calling at&amp;t right now to chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>308973246659829760</td>\n",
       "      <td>at&amp;t would never close down the store il even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>317674948388859904</td>\n",
       "      <td>If you need a phone or your looking to switch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>294841812416352256</td>\n",
       "      <td>@_ladytheGREAT lol hell naw Im Gucci Im good w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>282895292259508224</td>\n",
       "      <td>I swear if at&amp;t it tries to make me switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>333698413294133248</td>\n",
       "      <td>@markguim no; actually considering switching t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>293951135235047424</td>\n",
       "      <td>@Myhstery @AiRNAYLOR_ @YouHaaLL i will i have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>324569662694232064</td>\n",
       "      <td>Still a chance the HTC one will hit Verizon st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>314435246382800896</td>\n",
       "      <td>T-Mobile switching over to a buy your own phon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>299772046982930432</td>\n",
       "      <td>2mrrw its official i will b with at&amp;t hello at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>321511069153628160</td>\n",
       "      <td>@X0Mariel_ Its the Skyrocket 2 . Im tired of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>342815656519036928</td>\n",
       "      <td>I hate them stupid at&amp;t commercials with those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>388009531995082752</td>\n",
       "      <td>Im tired of at&amp;t man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>317629377112317952</td>\n",
       "      <td>@Kinny Yes...that. Switching to Verizon in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>325779375528235008</td>\n",
       "      <td>I have to admit T-Mobile works way better than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>293845805901697024</td>\n",
       "      <td>My phone bill was due on the 13th an Verizon a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>327136409293033472</td>\n",
       "      <td>@OneJaredNewman @TheRomit Have old at&amp;t plan 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>293608793877647360</td>\n",
       "      <td>Dont go to at&amp;t ! Well if you like being on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>306554953588154368</td>\n",
       "      <td>Been with at&amp;t for a few months but my other l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>304814641253605376</td>\n",
       "      <td>@Jaime_Rivera So if unlocks become legal; woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>344928057842356224</td>\n",
       "      <td>@orbyorb Yeah I noticed. at&amp;t has good LTE spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>397365336304934912</td>\n",
       "      <td>.@katetscott @RayWoodson680 @KNBR #SFGiants re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>315206668591960064</td>\n",
       "      <td>@cmdman I might switch to a T-Mobile plan whil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>311461224879644672</td>\n",
       "      <td>Maaaan i cnt wait til my contract up wit Veriz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>408275840099958784</td>\n",
       "      <td>Goodbye #verizon. Going with T-mobile. No unli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>357595876551688192</td>\n",
       "      <td>Anywho cant wait to leave verizon that means m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tid                                               text\n",
       "0     325958497122529280    Anybody wanna buy a Verizon iPhone 4s; for $125\n",
       "1     324539657436143616  When will Nokia Lumia 920 at&t exclusivity end...\n",
       "2     327288844758171648  at&t! @Ms_Marshall: Im legit bouta switch back...\n",
       "3     347214658379317248                 wind is a HUGE factor at at&t park\n",
       "4     395868806867742720  My phone still fucked up. Taking this shit to ...\n",
       "5     388716746900647936  @SuperSteveHixon still want to assume those 2 ...\n",
       "6     398102041668509696  If the season ended today; the 49ers would be ...\n",
       "7     326852829237088256   Switching from at&t to t mobile @ROLLthemTREESup\n",
       "8     301391249145991168               Fuck you; verizon. Im done with you.\n",
       "9     431091671192076288  @Right_UpMaiale @Emma_Spic3 I keep going over ...\n",
       "10    343919820003221504  @Uppity1 If you cancel your Verizon service; t...\n",
       "11    391309243703640064  @Crislex: Guess what @comcast; Im in the phone...\n",
       "12    324296820572442624  Verizon;I would love to know why my bill is 15...\n",
       "13    312720777331998720  You have reached the Verizon voicemail of 2 0 ...\n",
       "14    311816375742128128  Just got off the phone with Verizon; that stup...\n",
       "15    419274183030214656  at&t is offering T-Mobile customers the opport...\n",
       "16    420692453558321152  So Ive Been W/ at&t for almost a week .. I lik...\n",
       "17    359362581955223552                     at&t be tryna get out on folks\n",
       "18    340587376080482304  My dream was to work at Verizon Wireless or Be...\n",
       "19    314918667018178560  @Net10_Wireless my cancellation fee from Veriz...\n",
       "20    287013445403033600  @UcantSTANme: @gripthaGRAIN why you switching ...\n",
       "21    294941404872978432  @SuchA_EffnLadi no hes talking about dropping ...\n",
       "22    406799341559742464  @Pocketnow I will be switching to Verizon Prep...\n",
       "23    424356419614158848  Final at&t solution: Switch to data unlimited ...\n",
       "24    429851650820276224  @VZWSupport Been there; done that. Verizon ref...\n",
       "25    331058399049576448  I was thinking about paying that 215 but I ref...\n",
       "26    334491330476638208  end of 1st Q. Spurs have 37-28 lead vs GSW in ...\n",
       "27    383386026934165504  Verizon wireless would be so tight butthole if...\n",
       "28    295616492219813888  i think im switching from verizon to metro onc...\n",
       "29    416943791535575040  I cant wait til my contract up at at&t Im sick...\n",
       "...                  ...                                                ...\n",
       "2904  310580189283373056  At the end of 2 periods of play; things are al...\n",
       "2905  320284000797200384  @ThierrymrFNF I wanna c if u can switch this v...\n",
       "2906  408037057508810752  at&t RT@Amazing__dae: Who selling a iPhone 5 a...\n",
       "2907  405889734343688192  Im on the other end of Verizon Center and can ...\n",
       "2908  335434857201164288  @Love_HateB im calling at&t right now to chang...\n",
       "2909  308973246659829760  at&t would never close down the store il even ...\n",
       "2910  317674948388859904  If you need a phone or your looking to switch ...\n",
       "2911  294841812416352256  @_ladytheGREAT lol hell naw Im Gucci Im good w...\n",
       "2912  282895292259508224  I swear if at&t it tries to make me switch to ...\n",
       "2913  333698413294133248  @markguim no; actually considering switching t...\n",
       "2914  293951135235047424  @Myhstery @AiRNAYLOR_ @YouHaaLL i will i have ...\n",
       "2915  324569662694232064  Still a chance the HTC one will hit Verizon st...\n",
       "2916  314435246382800896  T-Mobile switching over to a buy your own phon...\n",
       "2917  299772046982930432  2mrrw its official i will b with at&t hello at...\n",
       "2918  321511069153628160  @X0Mariel_ Its the Skyrocket 2 . Im tired of i...\n",
       "2919  342815656519036928  I hate them stupid at&t commercials with those...\n",
       "2920  388009531995082752                               Im tired of at&t man\n",
       "2921  317629377112317952  @Kinny Yes...that. Switching to Verizon in the...\n",
       "2922  325779375528235008  I have to admit T-Mobile works way better than...\n",
       "2923  293845805901697024  My phone bill was due on the 13th an Verizon a...\n",
       "2924  327136409293033472  @OneJaredNewman @TheRomit Have old at&t plan 2...\n",
       "2925  293608793877647360  Dont go to at&t ! Well if you like being on th...\n",
       "2926  306554953588154368  Been with at&t for a few months but my other l...\n",
       "2927  304814641253605376  @Jaime_Rivera So if unlocks become legal; woul...\n",
       "2928  344928057842356224  @orbyorb Yeah I noticed. at&t has good LTE spe...\n",
       "2929  397365336304934912  .@katetscott @RayWoodson680 @KNBR #SFGiants re...\n",
       "2930  315206668591960064  @cmdman I might switch to a T-Mobile plan whil...\n",
       "2931  311461224879644672  Maaaan i cnt wait til my contract up wit Veriz...\n",
       "2932  408275840099958784  Goodbye #verizon. Going with T-mobile. No unli...\n",
       "2933  357595876551688192  Anywho cant wait to leave verizon that means m...\n",
       "\n",
       "[2934 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "sql = \"\"\"\n",
    "select tweets.tid, tweets.text from tweets join churn on churn.tid = tweets.tid \n",
    "where (tweets.text like '%AT&T%') \n",
    "or (tweets.text like '%Verizon%')\n",
    "or (tweets.text like '%switch%')\n",
    "and (churn.\"set\" like 'training')\n",
    "\"\"\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "res = cursor.fetchall()\n",
    "cursor.close()\n",
    "res_2_2 = pd.DataFrame(res)\n",
    "res_2_2[0] = res_2_2[0].astype(np.int)\n",
    "res_2_2.columns = ['tid', 'text']\n",
    "res_2_2.to_csv('ID_460490151_Q_2_2.csv', index=False)\n",
    "res_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identify Churning Customers via Logistic Regression (Total 10 Marks)\n",
    "\n",
    "Train a Logistic Regression Classifier to identify tweets from churning customers\n",
    "\n",
    "Requirements\n",
    "- The original features must be the tweet text data\n",
    "- Use dimension reduction to reduce the dimensionality of the problem. In class you have learnt about PCA. However PCA will not work for TF or TF-IDF data as it is sparse. You must find an alternative method in scikit-learn that works with sparse data.\n",
    "- Maximum of 5 components\n",
    "\n",
    "In Q2.3.5 your marks will be assigned based on your classifiers performance on the hidden set. Make sure you tune your model thoroughly in section Q2.3.3.\n",
    "\n",
    "#### 2.3.1 Transform Features (1.5 Marks)\n",
    "\n",
    "Given the original text data, use an sklearn vectoriser to convert the text to a numeric representation.\n",
    "\n",
    "Output your fitted vectoriser as a pickle file.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_3_1.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sqlite3\n",
    "\n",
    "cnx = sqlite3.connect(\"tweets.db\")\n",
    "tweets_data = pd.read_sql(\"SELECT*From tweets\",cnx)\n",
    "train_data = pd.read_sql(\"SELECT*From churn\",cnx)\n",
    "train_training_data = train_data[train_data['set']==\"training\"]\n",
    "train_merg = train_training_data.merge(tweets_data,on='tid',how='left').dropna()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train_merg['text'].isnull()\n",
    "train_merg_dropna= train_merg.dropna()\n",
    "train_merg = train_merg_dropna\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "corpus = train_merg['text']\n",
    "corpus = corpus.dropna()\n",
    "tfidf_transformer.fit_transform(corpus)\n",
    "result_2_3 = tfidf_transformer.transform(corpus)\n",
    "dense_2_3 = result_2_3.todense()\n",
    "dense_2_3\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_1.pickle\".format(460490151)\n",
    "s = pickle.dump(tfidf_transformer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will save your Transformer/Vectoriser object to a file\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_1.pickle\".format(460490151)\n",
    "\n",
    "# MYTRANSFORMEROBJECT must be a sklearn transformer or vectoriser\n",
    "s = pickle.dump(tfidf_transformer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dimension Reduction (1.5 Marks)\n",
    "\n",
    "Reduce the dimensionality of your features to a maximum of 5 components.\n",
    "\n",
    "Output your fitted dimensionality reducing object as a pickle file.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_3_2.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "cnx = sqlite3.connect(\"tweets.db\")\n",
    "tweets_data = pd.read_sql(\"SELECT*From tweets\",cnx)\n",
    "train_data = pd.read_sql(\"SELECT*From churn\",cnx)\n",
    "train_training_data = train_data[train_data['set']==\"training\"]\n",
    "train_merg = train_training_data.merge(tweets_data,on='tid',how='left').dropna()\n",
    "\n",
    "train_merg['text'].isnull()\n",
    "train_merg_dropna= train_merg.dropna()\n",
    "train_merg = train_merg_dropna\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "corpus = train_merg['text']\n",
    "corpus = corpus.dropna()\n",
    "tfidf_transformer.fit_transform(corpus)\n",
    "result_2_3 = tfidf_transformer.transform(corpus)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "reduced_d = TruncatedSVD(n_components = 5)\n",
    "reduced_d.fit(result_2_3)\n",
    "\n",
    "reduced_d1 = reduced_d.transform(result_2_3)\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_2.pickle\".format(460490151)\n",
    "\n",
    "s = pickle.dump(reduced_d, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will save your Dimensionality Reducer object to a file\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_2.pickle\".format(460490151)\n",
    "\n",
    "# MYREDUCEROBJECT must be a valid dimensionality reducer from sklearn\n",
    "s = pickle.dump(reduced_d, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Tuning (2 Marks)\n",
    "\n",
    "Tune your model hyper-parameters for best performance. Make sure to tune thoroughly!\n",
    "\n",
    "Output your fitted GridSearchCV or RandomisedSearchCV object as a pickle file.\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_3_3.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_intercept': False, 'C': 0.001, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "cnx = sqlite3.connect(\"tweets.db\")\n",
    "tweets_data = pd.read_sql(\"SELECT*From tweets\",cnx)\n",
    "train_data = pd.read_sql(\"SELECT*From churn\",cnx)\n",
    "train_training_data = train_data[train_data['set']==\"training\"]\n",
    "train_merg = train_training_data.merge(tweets_data,on='tid',how='left').dropna()\n",
    "d1=train_training_data['churn']\n",
    "\n",
    "#train_merg = pd.get_dummies(train_merg,drop_first=True)\n",
    "\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#np.random.seed(0)\n",
    "#x_data = train_training_data\n",
    "#y_data = train_merg['churn']\n",
    "#x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_1 = LogisticRegression()\n",
    "model_1.fit(reduced_d1[877:],d1[878:])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set up the grid of parameters to search \n",
    "param_grid = {\n",
    "    'C': np.linspace(1e-3,1e3,2000), \n",
    "    'penalty':['l2'],\n",
    "    'fit_intercept':[False],\n",
    "}\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(LogisticRegression(),param_grid)\n",
    "# Do the grid search\n",
    "grid_search.fit(reduced_d1[877:],d1[878:])\n",
    "# print(grid_search.best_params_)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_3.pickle\".format(YOURSTUDENTID)\n",
    "\n",
    "# MYGRIDSEARCHOBJECT must be GridSearchCV or RandomisedSearchCV\n",
    "s = pickle.dump(grid_search, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will save your GridSearchCV or RandomisedSearchCV to a file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Output Model (1 Marks)\n",
    "\n",
    "Output your trained logistic regression model as a pickle file. In the next part you will be competing against other students. So make sure you tune your model as best you can!\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_3_4.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "cnx = sqlite3.connect(\"tweets.db\")\n",
    "tweets_data = pd.read_sql(\"SELECT*From tweets\",cnx)\n",
    "train_data = pd.read_sql(\"SELECT*From churn\",cnx)\n",
    "train_training_data = train_data[train_data['set']==\"training\"]\n",
    "train_merg = train_training_data.merge(tweets_data,on='tid',how='left').dropna()\n",
    "\n",
    "train_merg = pd.get_dummies(train_merg,drop_first=True)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "#x_data = train_merg[train_merg.columns.difference(['churn'])]\n",
    "#y_data = train_merg['churn']\n",
    "#x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.3)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_1 = LogisticRegression(C=0.001)\n",
    "MYLOGISTICREGRESSION=model_1.fit(reduced_d1[877:],d1[878:])\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = \"ID_{0}_Q_2_3_4.pickle\".format(460490151)\n",
    "\n",
    "# MYLOGISTICREGRESSION must be of type sklearn.linear_model.LogisticRegression\n",
    "s = pickle.dump(model_1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Predicting Churn for the Hidden Customers (4 Marks)\n",
    "\n",
    "We will assign marks to this question based on the relative performance of each students classifier. You must try and tune your classifier in Question 2.3 as best you can!\n",
    "\n",
    "Output your predictions as a two column .csv file with the following format:\n",
    "\n",
    "| tid  |  Churn |\n",
    "|---|---|\n",
    "| tweet_id1  | 0  |\n",
    "| tweet_id2  | 1  |\n",
    "| tweet_id3  | 0  |\n",
    "| ...  | ...  |\n",
    "\n",
    "where pred1 is the predicted class i.e. 1 is \"Churn\" and 0 is \"Not churn\".\n",
    "\n",
    "FILENAME: ID_STUDENTID_Q_2_3_5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_sql(\"SELECT*From tweets\",cnx)\n",
    "churn_data = pd.read_sql(\"SELECT*From churn\",cnx)\n",
    "test_data = churn_data[churn_data['set']==\"hidden\"]\n",
    "test_merg = test_data.merge(tweets_data,on='tid',how='left')\n",
    "\n",
    "# p1 = model_1.predict(reduced_d1[:877])\n",
    "test_merg\n",
    "test_pred = pd.DataFrame(\n",
    "    model_1.predict(reduced_d.transform(tfidf_transformer.transform(test_merg.text))),\n",
    "    columns=['Churn'],\n",
    "    index=test_merg.index,\n",
    ")\n",
    "test_pred['tid'] = test_merg.tid.astype(np.int)\n",
    "test_pred = test_pred.sort_index(axis=1, ascending=False)\n",
    "test_pred.to_csv('ID_460490151_Q_2_3_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Prediction Competition (Total 10 Marks)\n",
    "\n",
    "We will assign marks to this question based on the relative performance of each students classifier.\n",
    "\n",
    "Your goal is to build the most accurate classification pipeline for the hidden data. You should do your own research to find suitable preprocessing steps and classifier. You are allowed to use **any preprocessing you like and any sklearn compatible classifier** i.e. it must support the following functions:\n",
    "- fit\n",
    "- predict\n",
    "\n",
    "You must output your classifier (as a pickle file) and predictions (as csv) using the format from Question 2.3.4 and 2.3.5.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "FILENAMES:\n",
    "- ID_{0}_Q_2_4_1.pickle\n",
    "- ID_{0}_Q_2_4_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_test_data():\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect('tweets.db')\n",
    "    sql = \"\"\"\n",
    "        select tweets.tid, date, text, churn, \"set\" from tweets join churn on churn.tid = tweets.tid\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    tw_data = pd.DataFrame(cursor.fetchall())\n",
    "    cursor.close()\n",
    "    tw_data.columns = ['tid', 'date', 'text', 'churn', 'set']\n",
    "    tw_data.tid = tw_data.tid.astype(np.int)\n",
    "    tw_data.date = pd.to_datetime(tw_data.date)\n",
    "    tw_data.pop('date')\n",
    "    \n",
    "    tw_train = tw_data[tw_data.set == 'training']\n",
    "    tw_test = tw_data[tw_data.set == 'hidden']\n",
    "    tw_train.pop('set')\n",
    "    tw_test.pop('set')\n",
    "\n",
    "    return tw_train, tw_test\n",
    "tw_train_valid, tw_test = get_train_test_data()\n",
    "\n",
    "tw_test_x = tw_test['text']\n",
    "tw_test_y = tw_test['churn']\n",
    "\n",
    "tw_train_valid_x = tw_train_valid['text']\n",
    "tw_train_valid_y = tw_train_valid['churn']\n",
    "\n",
    "tw_train_x, tw_valid_x, tw_train_y, tw_valid_y = train_test_split(tw_train_valid_x, tw_train_valid_y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "MOST_RELATE_WORD_COUNT = 100\n",
    "\n",
    "def get_relate_cv():\n",
    "    pat = re.compile('[@,;.*/!?#]')\n",
    "    replac_sym = re.compile(pat)\n",
    "\n",
    "    freq_cv = TfidfVectorizer()\n",
    "    sum_text = tw_train_x.map(lambda i: re.sub(pat, ' ', i)).sum().lower()\n",
    "    ct = collections.Counter(sum_text.split())\n",
    "    for k in ct.copy():\n",
    "        if ct[k] <= 2:\n",
    "            ct.pop(k)\n",
    "    freq_cv.fit(list(ct.keys()))\n",
    "    \n",
    "    cv = TfidfVectorizer()\n",
    "    train_feature = pd.DataFrame(freq_cv.transform(tw_train_x).toarray(), index=tw_train_x.index)\n",
    "    train_feature.columns = freq_cv.get_feature_names()\n",
    "    word_corr = (train_feature.corrwith(tw_train_y) - train_feature.corrwith(-tw_train_y+1)).dropna()\n",
    "    relate_words = list(word_corr.sort_values(ascending=False).index)\n",
    "    _bench = int(MOST_RELATE_WORD_COUNT/2)\n",
    "    choice_words = relate_words[:_bench] + relate_words[-_bench:]\n",
    "    cv.fit(choice_words)\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "cv = get_relate_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8362760834670947\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.94      0.90       490\n",
      "        1.0       0.67      0.45      0.54       133\n",
      "\n",
      "avg / total       0.82      0.84      0.82       623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "train_fea = pd.DataFrame(cv.transform(tw_train_x).toarray(), columns=cv.get_feature_names())\n",
    "clf.fit(train_fea, tw_train_y)\n",
    "valid_fea = pd.DataFrame(cv.transform(tw_valid_x).toarray(), columns=cv.get_feature_names())\n",
    "# print(clf.score(valid_fea, tw_valid_y))\n",
    "# print(classification_report(tw_valid_y, clf.predict(valid_fea)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test_fea = pd.DataFrame(cv.transform(tw_test_x).toarray(), columns=cv.get_feature_names())\n",
    "predict_test = pd.DataFrame(clf.predict(test_fea).astype(np.int), index=tw_test.tid, columns=['Churn'])\n",
    "predict_test.to_csv('ID_460490151_Q_2_4_1.csv')\n",
    "\n",
    "with open('ID_460490151_Q_2_4_1.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
